{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 13:20:43.104 | INFO     | __main__:<module>:7 - True\n",
      "2023-09-29 13:20:43.115 | INFO     | __main__:<module>:12 - GPU Name: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "\n",
    "# check if gpu available\n",
    "logger.info(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.has_cuda else \"cpu\")\n",
    "# print gpu name\n",
    "if device:\n",
    "    logger.info(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    logger.info('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-24 04:25:39.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mX shape: torch.Size([256, 784]) // X min: 0.0 X max: 1.0 \u001b[0m\n",
      "\u001b[32m2023-09-24 04:25:39.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mY shape: torch.Size([256])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnist_transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "dataset_path = \"../datasets/\"\n",
    "train_set = MNIST(dataset_path, train=True, transform=mnist_transform, download=True)\n",
    "val_set = MNIST(dataset_path, train=False, transform=mnist_transform, download=True)\n",
    "\n",
    "batch_size=256\n",
    "train_loader = DataLoader(train_set, batch_size, shuffle=True, pin_memory=True, num_workers=1)\n",
    "val_loader = DataLoader(val_set, batch_size, shuffle=True, pin_memory=True, num_workers=1)\n",
    "\n",
    "for batch_idx, (x,y) in enumerate(train_loader):\n",
    "    logger.info(f\"X shape: {x.shape} // X min: {x.min()} X max: {x.max()} \")\n",
    "    logger.info(f\"Y shape: {y.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import sigmoid\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.x1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.x2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.variance = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        f = self.act(self.x1(x))\n",
    "        f = self.act(self.x2(f))\n",
    "        mean = self.mean(f)\n",
    "        variance = self.variance(f)\n",
    "\n",
    "        return mean, variance\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.x1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.x2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.x3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "    \n",
    "    def forward(self, l):\n",
    "        f = self.act(self.x1(l))\n",
    "        f = self.act(self.x2(f))\n",
    "        output = sigmoid(self.act(self.x3(f)))\n",
    "\n",
    "        return output\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=input_dim)\n",
    "\n",
    "    def sample_randn(self, mean, var):\n",
    "        r = torch.randn_like(var).to(device)\n",
    "        z = mean + r*var  # z = m*r + s generates samples from normal distribution with mean(m) and sigma(s)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        z = self.sample_randn(mean, torch.exp(0.5*log_var)) # 0.5 since we're converting variance to sigma (var=pow(sigma, 2))\n",
    "        return self.decoder(z), mean, log_var\n",
    "    \n",
    "# model parameters\n",
    "input_dim=784 #NOTE: change for different image size (28x28=784)\n",
    "hidden_dim=512\n",
    "latent_dim=256\n",
    "vae_model = VAE(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=latent_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS and OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "optim = Adam(vae_model.parameters(), lr=1.e-3)\n",
    "\n",
    "bce_loss = BCELoss(reduction='sum')\n",
    "\n",
    "def loss_func(pred, gt, mean, log_var):\n",
    "    \n",
    "    def generative_loss(pred, gt):\n",
    "        return nn.functional.binary_cross_entropy(pred, gt, reduction='sum')\n",
    "    \n",
    "    # taken from here: https://medium.com/@outerrencedl/variational-autoencoder-and-a-bit-kl-divergence-with-pytorch-ce04fd55d0d7\n",
    "    def kld_loss(m, lv):\n",
    "        return 0.5*torch.sum(m.pow(2)+lv.exp()-1-lv)\n",
    "    \n",
    "    return generative_loss(pred, gt) + kld_loss(mean, log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-24 04:25:39.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 0 / Batch idx: 0 / loss: 553.79340\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-24 04:25:46.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 0 / Batch idx: 200 / loss: 192.59839\u001b[0m\n",
      "\u001b[32m2023-09-24 04:25:47.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 0 / Mean loss: 214.69221 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:25:47.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 1 / Batch idx: 0 / loss: 193.41066\u001b[0m\n",
      "\u001b[32m2023-09-24 04:25:54.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 1 / Batch idx: 200 / loss: 167.39661\u001b[0m\n",
      "\u001b[32m2023-09-24 04:25:56.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 1 / Mean loss: 180.24735 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:25:56.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 2 / Batch idx: 0 / loss: 167.62863\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:03.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 2 / Batch idx: 200 / loss: 149.13235\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:04.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 2 / Mean loss: 153.78649 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:04.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 3 / Batch idx: 0 / loss: 144.23686\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:11.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 3 / Batch idx: 200 / loss: 131.49129\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:13.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 3 / Mean loss: 137.33454 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:13.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 4 / Batch idx: 0 / loss: 135.40349\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:20.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 4 / Batch idx: 200 / loss: 130.71489\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:21.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 4 / Mean loss: 128.70159 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:21.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 5 / Batch idx: 0 / loss: 128.86646\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:28.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 5 / Batch idx: 200 / loss: 122.51201\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:30.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 5 / Mean loss: 124.08241 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:30.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 6 / Batch idx: 0 / loss: 122.34517\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:37.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 6 / Batch idx: 200 / loss: 116.34616\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:38.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 6 / Mean loss: 120.27586 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:38.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 7 / Batch idx: 0 / loss: 119.20495\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:45.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 7 / Batch idx: 200 / loss: 118.22375\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:46.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 7 / Mean loss: 117.63571 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:47.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 8 / Batch idx: 0 / loss: 111.65895\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:54.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 8 / Batch idx: 200 / loss: 117.06439\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:55.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 8 / Mean loss: 115.82290 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:26:55.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 9 / Batch idx: 0 / loss: 113.60677\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:02.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 9 / Batch idx: 200 / loss: 112.95343\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:03.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 9 / Mean loss: 114.55733 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:03.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 10 / Batch idx: 0 / loss: 113.84430\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:11.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 10 / Batch idx: 200 / loss: 110.45897\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:12.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 10 / Mean loss: 113.38969 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:12.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 11 / Batch idx: 0 / loss: 113.55402\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:19.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 11 / Batch idx: 200 / loss: 108.68854\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:20.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 11 / Mean loss: 112.22347 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:20.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 12 / Batch idx: 0 / loss: 109.43579\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:28.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 12 / Batch idx: 200 / loss: 113.97467\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:29.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 12 / Mean loss: 111.27343 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:29.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 13 / Batch idx: 0 / loss: 109.35143\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:36.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 13 / Batch idx: 200 / loss: 110.53604\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:37.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 13 / Mean loss: 110.54215 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:37.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 14 / Batch idx: 0 / loss: 110.47934\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:44.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 14 / Batch idx: 200 / loss: 112.46135\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:46.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 14 / Mean loss: 109.92504 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:46.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 15 / Batch idx: 0 / loss: 108.90286\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:53.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 15 / Batch idx: 200 / loss: 110.60387\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:54.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 15 / Mean loss: 109.15703 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:27:54.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 16 / Batch idx: 0 / loss: 107.99259\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:01.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 16 / Batch idx: 200 / loss: 106.33727\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:02.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 16 / Mean loss: 108.65430 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:03.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 17 / Batch idx: 0 / loss: 109.35358\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:10.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 17 / Batch idx: 200 / loss: 110.59975\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:11.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 17 / Mean loss: 108.06456 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:11.502\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 18 / Batch idx: 0 / loss: 109.53064\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:18.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 18 / Batch idx: 200 / loss: 105.55795\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:19.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 18 / Mean loss: 107.63434 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:19.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 19 / Batch idx: 0 / loss: 110.09570\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:27.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 19 / Batch idx: 200 / loss: 108.29192\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:28.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 19 / Mean loss: 107.10854 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:28.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 20 / Batch idx: 0 / loss: 105.24721\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:35.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 20 / Batch idx: 200 / loss: 109.90586\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:36.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 20 / Mean loss: 106.69512 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:36.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 21 / Batch idx: 0 / loss: 108.26578\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:44.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 21 / Batch idx: 200 / loss: 105.90870\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:45.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 21 / Mean loss: 106.40744 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:45.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 22 / Batch idx: 0 / loss: 103.67004\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:52.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 22 / Batch idx: 200 / loss: 103.66172\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:54.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 22 / Mean loss: 105.99993 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:28:54.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 23 / Batch idx: 0 / loss: 105.09229\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:01.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 23 / Batch idx: 200 / loss: 106.39010\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:02.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 23 / Mean loss: 105.72427 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:02.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 24 / Batch idx: 0 / loss: 108.71829\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:09.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 24 / Batch idx: 200 / loss: 105.84274\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:10.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 24 / Mean loss: 105.43294 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:11.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 25 / Batch idx: 0 / loss: 107.31770\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:18.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 25 / Batch idx: 200 / loss: 103.57735\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:19.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 25 / Mean loss: 105.08919 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:19.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 26 / Batch idx: 0 / loss: 105.55037\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:26.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 26 / Batch idx: 200 / loss: 103.24920\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:27.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 26 / Mean loss: 104.88907 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:27.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 27 / Batch idx: 0 / loss: 105.68300\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:34.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 27 / Batch idx: 200 / loss: 106.27293\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:36.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 27 / Mean loss: 104.51880 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:36.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 28 / Batch idx: 0 / loss: 103.66704\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:43.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 28 / Batch idx: 200 / loss: 101.83942\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:44.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 28 / Mean loss: 104.35075 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:44.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 29 / Batch idx: 0 / loss: 106.93813\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:51.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 29 / Batch idx: 200 / loss: 104.15285\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:52.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 29 / Mean loss: 104.11141 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:29:53.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 30 / Batch idx: 0 / loss: 102.34442\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:00.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 30 / Batch idx: 200 / loss: 103.83907\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:01.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 30 / Mean loss: 103.86663 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:01.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 31 / Batch idx: 0 / loss: 104.08034\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:08.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 31 / Batch idx: 200 / loss: 105.28897\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:09.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 31 / Mean loss: 103.73017 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:10.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 32 / Batch idx: 0 / loss: 103.61525\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:17.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 32 / Batch idx: 200 / loss: 102.86433\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:18.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 32 / Mean loss: 103.51723 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:18.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 33 / Batch idx: 0 / loss: 105.67970\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:25.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 33 / Batch idx: 200 / loss: 105.39053\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:26.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 33 / Mean loss: 103.38295 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:26.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 34 / Batch idx: 0 / loss: 103.25595\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:34.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 34 / Batch idx: 200 / loss: 103.09531\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:35.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 34 / Mean loss: 103.21271 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:35.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 35 / Batch idx: 0 / loss: 106.61514\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:42.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 35 / Batch idx: 200 / loss: 102.72382\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:43.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 35 / Mean loss: 103.05484 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:43.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 36 / Batch idx: 0 / loss: 105.72227\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:50.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 36 / Batch idx: 200 / loss: 104.45464\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:52.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 36 / Mean loss: 102.92419 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:52.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 37 / Batch idx: 0 / loss: 102.33757\u001b[0m\n",
      "\u001b[32m2023-09-24 04:30:59.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 37 / Batch idx: 200 / loss: 101.43256\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:00.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 37 / Mean loss: 102.81595 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:00.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 38 / Batch idx: 0 / loss: 102.44281\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:07.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 38 / Batch idx: 200 / loss: 102.95341\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:09.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 38 / Mean loss: 102.68704 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:09.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 39 / Batch idx: 0 / loss: 102.33578\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:16.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 39 / Batch idx: 200 / loss: 97.62376\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:17.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 39 / Mean loss: 102.57839 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:17.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 40 / Batch idx: 0 / loss: 102.98569\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:24.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 40 / Batch idx: 200 / loss: 100.67970\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:26.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 40 / Mean loss: 102.35573 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:26.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 41 / Batch idx: 0 / loss: 104.10234\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:33.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 41 / Batch idx: 200 / loss: 102.94543\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:34.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 41 / Mean loss: 102.28405 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:34.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 42 / Batch idx: 0 / loss: 104.45927\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:41.829\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 42 / Batch idx: 200 / loss: 103.35316\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:43.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 42 / Mean loss: 102.21261 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:43.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 43 / Batch idx: 0 / loss: 98.93758\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:50.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 43 / Batch idx: 200 / loss: 103.02835\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:51.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 43 / Mean loss: 102.04687 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:51.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 44 / Batch idx: 0 / loss: 103.53322\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:58.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 44 / Batch idx: 200 / loss: 99.35580\u001b[0m\n",
      "\u001b[32m2023-09-24 04:31:59.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 44 / Mean loss: 101.98599 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:00.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 45 / Batch idx: 0 / loss: 101.49447\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:07.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 45 / Batch idx: 200 / loss: 102.82278\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:08.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 45 / Mean loss: 101.84546 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:08.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 46 / Batch idx: 0 / loss: 102.31207\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:15.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 46 / Batch idx: 200 / loss: 101.86587\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:16.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 46 / Mean loss: 101.77023 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:16.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 47 / Batch idx: 0 / loss: 98.82076\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:23.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 47 / Batch idx: 200 / loss: 103.48259\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:25.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 47 / Mean loss: 101.71612 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:25.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 48 / Batch idx: 0 / loss: 103.43473\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:32.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 48 / Batch idx: 200 / loss: 99.89307\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:33.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 48 / Mean loss: 101.58200 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:33.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 49 / Batch idx: 0 / loss: 99.92097\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:40.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mEpoch: 49 / Batch idx: 200 / loss: 100.15218\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:41.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m------------   Epoch: 49 / Mean loss: 101.52188 -------------\u001b[0m\n",
      "\u001b[32m2023-09-24 04:32:41.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mTraining complete: Total epochs: 50 / Final Mean loss: 101.52188\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_sum = 0.\n",
    "    for batch_idx, (x,_) in enumerate(train_loader):\n",
    "        x=x.to(device=device)\n",
    "        optim.zero_grad()\n",
    "        y_hat, m, v = vae_model.forward(x)\n",
    "        loss = loss_func(y_hat, x, m, v)\n",
    "        loss_sum +=loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if batch_idx%200==0:\n",
    "            logger.info(f\"Epoch: {epoch} / Batch idx: {batch_idx} / loss: {loss.item()/batch_size:0.5f}\")\n",
    "    logger.info(f\"------------   Epoch: {epoch} / Mean loss: {loss_sum/(len(train_loader)*batch_size):0.5f} -------------\")\n",
    "logger.info(f\"Training complete: Total epochs: {epoch+1} / Final Mean loss: {loss_sum/(len(train_loader)*batch_size):0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "def plot_img(x, idx=0):\n",
    "    x = x.view(batch_size, 28, 28)\n",
    "    fig=plt.figure()\n",
    "    plt.imshow(x[idx].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-24 05:09:51.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mGT: 3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x,y) in enumerate(val_loader):\n",
    "        x=x.to(device=device)\n",
    "        output, _, _ = vae_model(x)\n",
    "        break\n",
    "\n",
    "random_idx = random.randint(0,batch_size)\n",
    "logger.info(f\"GT: {y[random_idx]}\")\n",
    "plot_img(output, random_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
